import os
import re
import time
import random
import json
import threading
import pandas as pd
import pdfplumber
import requests

from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from tenacity import retry, stop_after_attempt, wait_exponential
from concurrent.futures import ThreadPoolExecutor, as_completed

# ========= æ ¸å¿ƒå‚æ•°é…ç½® ==========
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    # è·¯å¾„é…ç½® (å·²æ›´æ–°ä¸ºä½ çš„æ–°é…ç½®)
    PROJECT_DIR = Path("/Volumes/ZimingYe/A_project")
    EXCEL_PATH = PROJECT_DIR / "oecd-ai-all-ai-policies.csv"
    SAVE_DIR = PROJECT_DIR / "0915-oecd-output_texts"
    PDF_SAVE_DIR = PROJECT_DIR / "0915-oecd-output_pdfs"
    CSV_OUTPUT = PROJECT_DIR / "0915-oecd_results.csv"
    URL_COLUMN = "Public access URL"
    
    # æ³¨æ„ï¼šè¿™é‡Œçš„è·¯å¾„éœ€è¦ç¡®ä¿ä¸åŒ…å«é¢å¤–çš„å¼•å·
    LOCAL_CHROMEDRIVER_PATH = "/opt/homebrew/bin/chromedriver"

    # çˆ¬è™«è¡Œä¸ºé…ç½®
    MAX_THREADS = 5
    PDF_DOWNLOAD_LIMIT = 5
    PAGE_LOAD_TIMEOUT = 60
    PDF_DOWNLOAD_TIMEOUT = 90
    RANDOM_DELAY_MIN = 3
    RANDOM_DELAY_MAX = 8

    # OECDç½‘ç«™ç‰¹å®šçš„AIå’Œæ²»ç†å…³é”®è¯
    AI_GOVERNANCE_KEYWORDS = [
        "artificial intelligence", "AI", "machine learning", "neural network",
        "deep learning", "automated decision", "algorithmic system",
        "data-driven", "intelligent system", "automated system",
        "AI governance", "AI policy", "AI strategy", "AI ethics",
        "digital transformation", "algorithmic accountability",
        "AI regulation", "AI guidelines", "responsible AI"
    ]

# ç”¨æˆ·ä»£ç†æ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

# çº¿ç¨‹é”
driver_lock = threading.Lock()

# --- æ–‡æœ¬æ¸…ç†å’Œå¤„ç†å‡½æ•° ---
def clean_text_for_csv(text):
    """
    å¢å¼ºç‰ˆæ–‡æœ¬æ¸…ç†ï¼Œç‰¹åˆ«é’ˆå¯¹æ”¿ç­–æ–‡æ¡£
    """
    if not text or not isinstance(text, str):
        return ""
    text = re.sub(r'[\n\r\f\v\x0b\x0c\t]+', ' ', text)
    text = re.sub(r'[\x00-\x08\x0e-\x1f\x7f-\x9f]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace('"', '""')
    return text

def contains_ai_governance_keywords(text):
    """æ£€æµ‹AIæ²»ç†ç›¸å…³å…³é”®è¯å¹¶è¿”å›åŒ¹é…ä¿¡æ¯"""
    if not text or not isinstance(text, str):
        return False
    text_lower = text.lower()
    matched_keywords = [k for k in Config.AI_GOVERNANCE_KEYWORDS if k.lower() in text_lower]

    if len(matched_keywords) >= 2:
        return f"é«˜åº¦ç›¸å…³ (åŒ¹é…{len(matched_keywords)}ä¸ªå…³é”®è¯: {', '.join(matched_keywords[:3])})"
    elif len(matched_keywords) == 1:
        return f"ç›¸å…³ (åŒ¹é…å…³é”®è¯: {matched_keywords[0]})"
    else:
        return False

# --- PDFæ–‡æœ¬æå–å¢å¼º ---
def extract_pdf_text_robust(pdf_path):
    """
    å¢å¼ºç‰ˆPDFæ–‡æœ¬æå–ï¼Œæä¾›å¤šç§ç­–ç•¥ä»¥æé«˜æˆåŠŸç‡ã€‚
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            texts = []
            for page in pdf.pages:
                page_text = ""
                # ç­–ç•¥ 1: æ ‡å‡†æ–‡æœ¬æå–
                page_text = page.extract_text()
                
                if not page_text or len(page_text.strip()) < 10:
                    # ç­–ç•¥ 2: å¸ƒå±€ä¿æŒæå–
                    page_text = page.extract_text(layout=True)
                
                if not page_text or len(page_text.strip()) < 10:
                    # ç­–ç•¥ 3: è¡¨æ ¼å’Œå›¾å½¢æå–ï¼ˆä½œä¸ºè¡¥å……ï¼‰
                    tables = page.extract_tables()
                    if tables:
                        for table in tables:
                            table_text = " | ".join([" ".join(str(cell) if cell else "" for cell in row) for row in table])
                            page_text += " " + table_text

                if page_text and len(page_text.strip()) > 10:
                    texts.append(page_text)
            
            full_text = " ".join(texts)
            if not full_text:
                return f"[WARNING] PDFè§£ææˆåŠŸä½†æœªèƒ½æå–æœ‰æ•ˆæ–‡æœ¬"
            
            return full_text
            
    except Exception as e:
        return f"[ERROR] PDFè§£æå¤±è´¥: {str(e)}"

# --- PDFä¸‹è½½ä¼˜åŒ– ---
def download_pdf_with_metadata(url, session, output_dir, url_index, page_info=None):
    """
    å¸¦å…ƒæ•°æ®çš„PDFä¸‹è½½å‡½æ•°ï¼Œå°†å…ƒæ•°æ®ä¿å­˜ä¸ºå•ç‹¬çš„JSONæ–‡ä»¶ã€‚
    """
    try:
        headers = {'User-Agent': random.choice(USER_AGENTS)}
        response = session.get(url, headers=headers, timeout=Config.PDF_DOWNLOAD_TIMEOUT, stream=True)
        
        if response.status_code != 200:
            return None, f"HTTPçŠ¶æ€ç : {response.status_code}"

        # æ£€æŸ¥å†…å®¹ç±»å‹å’Œæ–‡ä»¶å¤´
        content_type = response.headers.get('content-type', '').lower()
        is_pdf_content = 'pdf' in content_type or response.content.startswith(b'%PDF')
        if not is_pdf_content:
            return None, f"ä¸æ˜¯æœ‰æ•ˆçš„PDFæ–‡ä»¶ï¼Œå†…å®¹ç±»å‹: {content_type}"

        # ç”Ÿæˆæ–‡ä»¶å
        policy_title = (page_info.get('policy_title', '') or url.split('/')[-1]).replace('/', '_')
        clean_title = re.sub(r'[\\/:*?"<>|.,;=+[\]]', '_', policy_title)[:50]
        filename = f"{url_index:04d}-{clean_title}.pdf"
        pdf_path = output_dir / filename

        with open(pdf_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'url': url,
            'filename': filename,
            'download_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'country': page_info.get('country'),
            'policy_title': page_info.get('policy_title'),
            'file_size': os.path.getsize(pdf_path)
        }
        metadata_path = pdf_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        return pdf_path, None
    except requests.exceptions.RequestException as e:
        return None, f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"
    except Exception as e:
        return None, f"ä¸‹è½½å¤±è´¥: {str(e)}"

# --- Seleniumå’Œæ ¸å¿ƒå¤„ç†å‡½æ•° ---
def init_chrome_driver_local():
    """åˆå§‹åŒ–Chrome Driverï¼Œæ·»åŠ æ›´å¤šé˜²æ£€æµ‹å’Œç¨³å®šæ€§é…ç½®"""
    print("ğŸš€ åˆå§‹åŒ–Chromeæµè§ˆå™¨...")
    if not Path(Config.LOCAL_CHROMEDRIVER_PATH).exists():
        print(f"âŒ ChromeDriveræ–‡ä»¶ä¸å­˜åœ¨: {Config.LOCAL_CHROMEDRIVER_PATH}")
        return None
    
    options = webdriver.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(f"--user-agent={random.choice(USER_AGENTS)}")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    
    prefs = {
        "download.default_directory": str(Config.PDF_SAVE_DIR),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "plugins.always_open_pdf_externally": True,
        "profile.default_content_setting_values.notifications": 2
    }
    options.add_experimental_option("prefs", prefs)
    
    try:
        service = Service(executable_path=Config.LOCAL_CHROMEDRIVER_PATH)
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(Config.PAGE_LOAD_TIMEOUT)
        print("âœ… Chromeæµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver
    except Exception as e:
        print(f"âŒ Chromeæµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def handle_oecd_dynamic_content(driver):
    """å¤„ç†åŠ¨æ€å†…å®¹åŠ è½½å’Œcookieså¼¹çª—"""
    try:
        try:
            cookie_accept = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Accept') or contains(text(), 'agree')]"))
            )
            cookie_accept.click()
            time.sleep(2)
        except TimeoutException:
            pass
        
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
    except Exception as e:
        print(f"åŠ¨æ€å†…å®¹å¤„ç†å¤±è´¥: {e}")

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=3, max=10))
def process_oecd_url(url, url_index):
    """ä¸“é—¨å¤„ç†OECD.aiç½‘ç«™URLçš„ä¼˜åŒ–å‡½æ•°"""
    print(f"ğŸŒ æ­£åœ¨å¤„ç†OECDé“¾æ¥: {url}")
    
    session = requests.Session()
    driver = None
    extracted_text = ""
    pdf_docs_count = 0
    
    try:
        with driver_lock:
            driver = init_chrome_driver_local()
        
        if not driver:
            raise Exception("æ— æ³•åˆå§‹åŒ–Chromeæµè§ˆå™¨")
        
        driver.get(url)
        handle_oecd_dynamic_content(driver)
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        cookies = driver.get_cookies()
        for cookie in cookies:
            session.cookies.set(cookie['name'], cookie['value'])
            
        page_info = {
            'country': re.search(r'country=(\w+)', url, re.I).group(1) if re.search(r'country=(\w+)', url, re.I) else 'unknown',
            'policy_title': soup.find('h1').get_text(strip=True) if soup.find('h1') else None
        }

        pdf_links = [urljoin(url, a['href']) for a in soup.find_all('a', href=True) if '.pdf' in a['href'].lower()]
        
        if pdf_links:
            print(f"ğŸ“„ å‘ç° {len(pdf_links)} ä¸ªPDFé“¾æ¥ï¼Œå¼€å§‹ä¸‹è½½...")
            
            successful_texts = []
            with ThreadPoolExecutor(max_workers=Config.MAX_THREADS) as executor:
                futures = [
                    executor.submit(download_pdf_with_metadata, pdf_url, session, Config.PDF_SAVE_DIR, url_index, page_info)
                    for pdf_url in pdf_links[:Config.PDF_DOWNLOAD_LIMIT]
                ]
                for future in as_completed(futures):
                    pdf_path, error = future.result()
                    if pdf_path:
                        text = extract_pdf_text_robust(pdf_path)
                        if not text.startswith("[ERROR]") and len(text.strip()) > 100:
                            successful_texts.append(text)
                            pdf_docs_count += 1
                            print(f"âœ… PDFä¸‹è½½å¹¶æå–æˆåŠŸ: {os.path.basename(pdf_path)}")
                        else:
                            print(f"âš ï¸ PDFå†…å®¹æå–é—®é¢˜: {error or 'å†…å®¹è¿‡å°‘'}")
                    else:
                        print(f"âŒ PDFä¸‹è½½å¤±è´¥: {error}")
            
            if successful_texts:
                extracted_text = "\n\n--- æ–‡æ¡£åˆ†éš” ---\n\n".join(successful_texts)
            else:
                extracted_text = f"[WARNING] æ— æ³•ä¸‹è½½æˆ–æå–ä»»ä½•PDFå†…å®¹ï¼Œå°†å°è¯•æå–ç½‘é¡µæ–‡æœ¬ã€‚"
        
        if not extracted_text or extracted_text.startswith("[WARNING]"):
            print("ğŸ“ æå–ç½‘é¡µæ–‡æœ¬ä½œä¸ºå¤‡é€‰...")
            for element in soup(["script", "style", "nav", "header", "footer", "aside", "form", "button", "img"]):
                element.decompose()
            
            main_content = soup.find('article') or soup.find(id='main-content') or soup.find('.main-content')
            if main_content:
                webpage_text = main_content.get_text(strip=True)
            else:
                webpage_text = soup.get_text(strip=True)
                
            extracted_text = (f"[å›½å®¶: {page_info['country']}]\n"
                              f"[æ”¿ç­–æ ‡é¢˜: {page_info['policy_title']}]\n\n"
                              f"{webpage_text}")
                              
    except Exception as e:
        extracted_text = f"[ERROR] OECD URLå¤„ç†å¤±è´¥: {str(e)}"
        
    finally:
        if driver:
            try:
                driver.quit()
            except Exception as e:
                print(f"å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")
                
    return clean_text_for_csv(extracted_text), pdf_docs_count

# --- ä¸»ç¨‹åº ---
def main():
    """ä¸»æ‰§è¡Œå‡½æ•° - æ•´åˆæ‰€æœ‰åŠŸèƒ½"""
    print("ğŸš€ å¯åŠ¨OECD.aiæ–‡æ¡£æŠ“å–å·¥å…·")
    
    for path in [Config.SAVE_DIR, Config.PDF_SAVE_DIR]:
        path.mkdir(parents=True, exist_ok=True)
    
    if not Config.EXCEL_PATH.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {Config.EXCEL_PATH}")
        return
        
    try:
        # è¯»å–æ–‡ä»¶ï¼Œæ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©ä¸åŒçš„è¯»å–æ–¹æ³•
        if Config.EXCEL_PATH.suffix.lower() == '.csv':
            df = pd.read_csv(Config.EXCEL_PATH)
        else:
            df = pd.read_excel(Config.EXCEL_PATH)
            
        df.columns = [c.strip() for c in df.columns]
        
        required_columns = ['ç¼–å·', Config.URL_COLUMN]
        # æ·»åŠ  'ç¼–å·' åˆ—ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
        if 'ç¼–å·' not in df.columns:
            df['ç¼–å·'] = range(1, len(df) + 1)
        
        if Config.URL_COLUMN not in df.columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {Config.URL_COLUMN}")
            return
            
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    results = []
    success_count, total_pdf_count = 0, 0
    total_urls = len(df)
    
    print(f"ğŸ“ˆ å¼€å§‹å¤„ç† {total_urls} ä¸ªURL...")
    
    for idx, row in df.iterrows():
        url = str(row[Config.URL_COLUMN]).strip()
        file_no = str(row['ç¼–å·']).zfill(4)
        
        if not url.startswith("http"):
            results.append({**row.to_dict(), "æå–æ–‡æœ¬": "[ERROR] æ— æ•ˆURL", "å¤„ç†çŠ¶æ€": "è·³è¿‡"})
            continue
            
        print(f"\n--- [{idx + 1}/{total_urls}] ç¼–å·: {file_no} ---")
        start_time = time.time()
        
        extracted_text, pdf_docs_count = process_oecd_url(url, idx)
        
        status, ai_relevance, display_text, text_length, filename = "", "", "", 0, f"{file_no}.txt"
        
        if extracted_text.startswith("[ERROR]"):
            status = "å¤±è´¥"
            ai_relevance = "å¤„ç†å¤±è´¥"
            display_text = extracted_text
            text_length = 0
            print(f"âŒ å¤„ç†å¤±è´¥: {extracted_text}")
        else:
            status = "æˆåŠŸ"
            if pdf_docs_count > 0:
                status += f"-{pdf_docs_count}ä¸ªPDF"
            else:
                status += "-ç½‘é¡µæ–‡æœ¬"
            
            try:
                with open(Config.SAVE_DIR / filename, "w", encoding="utf-8") as f:
                    f.write(extracted_text)
                print(f"ğŸ’¾ æ–‡æœ¬å·²ä¿å­˜: {filename}")
            except Exception as e:
                extracted_text = f"[ERROR] æ–‡æœ¬ä¿å­˜å¤±è´¥: {e}"
                status = "å¤±è´¥-ä¿å­˜å¼‚å¸¸"
            
            ai_relevance = contains_ai_governance_keywords(extracted_text) or "ä¸ç›¸å…³"
            text_length = len(extracted_text)
            display_text = f"æ–‡æœ¬å†…å®¹å·²ä¿å­˜åˆ°æ–‡ä»¶ {filename}"
            if text_length < 500: # é•¿åº¦è¾ƒçŸ­æ—¶æ˜¾ç¤ºå†…å®¹
                display_text = extracted_text
            
            success_count += 1
            total_pdf_count += pdf_docs_count
            print(f"âœ… å¤„ç†æˆåŠŸ, æ–‡æœ¬é•¿åº¦: {text_length}, AIç›¸å…³æ€§: {ai_relevance}")

        processing_time = time.time() - start_time
        
        results.append({
            **row.to_dict(),
            "æå–æ–‡æœ¬": display_text,
            "AIæ²»ç†ç›¸å…³æ€§": ai_relevance,
            "æ–‡ä»¶å": filename,
            "å¤„ç†çŠ¶æ€": status,
            "PDFæ–‡æ¡£æ•°": pdf_docs_count,
            "å¤„ç†æ—¶é—´(ç§’)": round(processing_time, 1),
            "æ–‡æœ¬é•¿åº¦": text_length
        })
        
        print(f"ğŸ“Š è¿›åº¦: {(idx + 1)/total_urls*100:.1f}% | æˆåŠŸ: {success_count} | PDFæ€»æ•°: {total_pdf_count}")
        time.sleep(random.uniform(Config.RANDOM_DELAY_MIN, Config.RANDOM_DELAY_MAX))

    try:
        result_df = pd.DataFrame(results)
        result_df.to_csv(Config.CSV_OUTPUT, index=False, encoding="utf-8-sig")
        print("\n" + "=" * 60)
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {Config.CSV_OUTPUT}")
        print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶ç›®å½•: {Config.SAVE_DIR}")
        print(f"ğŸ“ PDFæ–‡ä»¶ç›®å½•: {Config.PDF_SAVE_DIR}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

if __name__ == "__main__":
    main()