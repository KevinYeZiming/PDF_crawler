import os
import re
import time
import random
import json
import threading
import pandas as pd
import pdfplumber
import requests
import logging
from typing import Optional, Tuple, Dict, List, Any

from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, ElementClickInterceptedException
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
from concurrent.futures import ThreadPoolExecutor, as_completed
# ç§»é™¤äº†æœªä½¿ç”¨çš„zipfileå’Œmimetypes

# ========= æ—¥å¿—é…ç½® (Logging Configuration) ==========
# è®¾ç½®æ—¥å¿—çº§åˆ«å’Œæ ¼å¼ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========= æ ¸å¿ƒå‚æ•°é…ç½® (Configuration Class) ==========
class Config:
    """
    é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°ã€‚
    è¯·æ ¹æ®ä½ çš„é¡¹ç›®å®é™…æƒ…å†µä¿®æ”¹ PROJECT_DIR å’Œå…¶ä»–è·¯å¾„ã€‚
    """
    # è·¯å¾„é…ç½®
    PROJECT_DIR = Path("/Volumes/ZimingYe/A_project/0928-è§„åˆ™é›†æ•°æ®")
    EXCEL_PATH = PROJECT_DIR / "/Volumes/ZimingYe/A_project/ç¬¬0è½®æ•°æ®é‡‡é›†/è§„åˆ™é›†æ•°æ®æŠ“å–.xlsx"
    SAVE_DIR = PROJECT_DIR / "0928-è§„åˆ™é›†æ•°æ®-output_texts"
    PDF_SAVE_DIR = PROJECT_DIR / "0928-è§„åˆ™é›†æ•°æ®-output_pdfs"
    CSV_OUTPUT = PROJECT_DIR / "0928-è§„åˆ™é›†æ•°æ®.csv"
    TEMP_DIR = PROJECT_DIR / "temp"
    
    # æ”¯æŒçš„è¾“å…¥æ–‡ä»¶æ ¼å¼
    SUPPORTED_FORMATS = ['.csv', '.xlsx', '.xls']
    
    # å¯èƒ½çš„URLåˆ—åï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼Œç”¨äºæ™ºèƒ½æŸ¥æ‰¾
    URL_COLUMN_CANDIDATES = [
        "Public access URL", "URL", "url", "link", "Link", "ç½‘å€", "é“¾æ¥",
        "Policy URL", "Document URL", "Source URL"
    ]
    
    # ChromeDriverè·¯å¾„ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œè¯·ç¡®ä¿ä½ çš„è·¯å¾„åŒ…å«åœ¨å†…æˆ–å·²æ·»åŠ åˆ°ç³»ç»ŸPATHï¼‰
    CHROMEDRIVER_PATHS = [
        "/opt/homebrew/bin/chromedriver",  # macOS Homebrew
        "/usr/local/bin/chromedriver",     # é€šç”¨è·¯å¾„
        "/usr/bin/chromedriver",           # Linux
        "chromedriver",                    # PATHä¸­
        "./chromedriver"                   # å½“å‰ç›®å½•
    ]

    # çˆ¬è™«è¡Œä¸ºé…ç½®
    MAX_THREADS = 3  # æœ€å¤§å¹¶å‘å¤„ç†URLæ•°é‡ï¼Œå»ºè®®ä¿æŒè¾ƒä½ä»¥æé«˜ç¨³å®šæ€§
    PDF_DOWNLOAD_LIMIT = 10  # æ¯ä¸ªURLæœ€å¤šä¸‹è½½çš„PDFæ–‡æ¡£æ•°
    PAGE_LOAD_TIMEOUT = 45  # é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    PDF_DOWNLOAD_TIMEOUT = 120  # PDFä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    RANDOM_DELAY_MIN = 2  # éšæœºå»¶è¿Ÿæœ€å°æ—¶é—´ï¼ˆç§’ï¼‰
    RANDOM_DELAY_MAX = 5  # éšæœºå»¶è¿Ÿæœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰
    MAX_RETRIES = 3  # ç½‘ç»œè¯·æ±‚å’Œæ ¸å¿ƒå¤„ç†çš„æœ€å¤§é‡è¯•æ¬¡æ•°
    
    # æ™ºèƒ½å¯¼èˆªé…ç½®
    ENABLE_SMART_NAVIGATION = True  # æ˜¯å¦å¯ç”¨æ™ºèƒ½å¯¼èˆªåŠŸèƒ½ï¼ˆä½¿ç”¨Seleniumé€’å½’æŸ¥æ‰¾AIå­é¡µé¢ï¼‰
    MAX_NAVIGATION_DEPTH = 2  # æœ€å¤§å¯¼èˆªæ·±åº¦ (0: ä»…å½“å‰é¡µ; 1: å½“å‰é¡µ+ä¸€å±‚å­é¡µé¢)
    MAX_AI_LINKS_PER_PAGE = 3  # æ¯é¡µæœ€å¤šè·Ÿè¸ªçš„AIç›¸å…³é“¾æ¥æ•°
    MIN_CONTENT_LENGTH = 200  # é¡µé¢å†…å®¹æœ€å°é•¿åº¦æ‰ä¿å­˜ï¼ˆé˜²æ­¢ä¿å­˜ç©ºé¡µæˆ–å¯¼èˆªé¡µï¼‰
    
    # å¼¹çª—å¤„ç†é…ç½®
    POPUP_DETECTION_TIMEOUT = 3  # å¼¹çª—æ£€æµ‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    MAX_POPUP_ATTEMPTS = 5  # æœ€å¤§å¼¹çª—å¤„ç†å°è¯•æ¬¡æ•°
    
    # æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆMBï¼‰
    MAX_PDF_SIZE_MB = 50

    # ç‰¹å®šçš„AIå’Œæ²»ç†å…³é”®è¯ï¼Œç”¨äºåˆ¤æ–­ç›¸å…³æ€§
    AI_GOVERNANCE_KEYWORDS = [
        "artificial intelligence", "AI", "machine learning", "neural network",
        "deep learning", "automated decision", "algorithmic system",
        "data-driven", "intelligent system", "automated system",
        "AI governance", "AI policy", "AI strategy", "AI ethics",
        "digital transformation", "algorithmic accountability",
        "AI regulation", "AI guidelines", "responsible AI",
        "digital policy", "technology policy", "innovation policy"
    ]

# é«˜è´¨é‡ç”¨æˆ·ä»£ç†æ±  (User Agents)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15"
]

# çº¿ç¨‹é”å’Œå…¨å±€å˜é‡
driver_lock = threading.Lock() # ç”¨äºä¿æŠ¤æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–è¿‡ç¨‹
session_cookies = {} # å­˜å‚¨ä¼šè¯cookies

# --- æ–‡ä»¶æ ¼å¼å…¼å®¹æ€§å¤„ç† (File Handling) ---
def detect_and_read_file(file_path: Path) -> pd.DataFrame:
    """æ™ºèƒ½æ£€æµ‹å¹¶è¯»å–å¤šç§æ ¼å¼çš„æ–‡ä»¶ï¼ˆCSV/Excelï¼‰"""
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    file_ext = file_path.suffix.lower()
    
    try:
        if file_ext == '.csv':
            # å°è¯•å¤šç§ç¼–ç è¯»å–CSV
            encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'iso-8859-1']
            for encoding in encodings:
                try:
                    # å°è¯•è¯»å–ï¼Œè®¾ç½®sep=None, engine='python' å°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦ï¼Œä½†é€šå¸¸é€—å·æ˜¯æ ‡å‡†
                    df = pd.read_csv(file_path, encoding=encoding)
                    logger.info(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼Œç¼–ç : {encoding}")
                    return df
                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue
            raise Exception("æ— æ³•è¯†åˆ«CSVæ–‡ä»¶ç¼–ç æˆ–æ ¼å¼")
            
        elif file_ext in ['.xlsx', '.xls']:
            # è¯»å–Excelæ–‡ä»¶
            engine = 'openpyxl' if file_ext == '.xlsx' else 'xlrd'
            df = pd.read_excel(file_path, engine=engine)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶: {file_ext}")
            return df
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        raise

def find_url_column(df: pd.DataFrame) -> Optional[str]:
    """æ™ºèƒ½æŸ¥æ‰¾åŒ…å«URLçš„åˆ—å"""
    df.columns = [str(c).strip() for c in df.columns]
    
    # ç­–ç•¥1: åŒ¹é…é¢„è®¾çš„å€™é€‰åˆ—å
    for candidate in Config.URL_COLUMN_CANDIDATES:
        if candidate in df.columns:
            logger.info(f"æ‰¾åˆ°URLåˆ—: {candidate}")
            return candidate
    
    # ç­–ç•¥2: æ£€æŸ¥åˆ—å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦åŒ…å«URLæ ¼å¼
    for col in df.columns:
        # æå–å‰10ä¸ªéç©ºå€¼è¿›è¡Œæ£€æŸ¥
        sample_values = df[col].dropna().head(10).astype(str)
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å€¼ä»¥ http:// æˆ– https:// å¼€å¤´
        if not sample_values.empty and sample_values.str.startswith(('http://', 'https://')).any():
            logger.info(f"é€šè¿‡å†…å®¹æ¨æ–­URLåˆ—: {col}")
            return col
    
    return None

# --- æ–‡æœ¬æ¸…ç†å’Œå¤„ç†å‡½æ•° (Text Processing) ---
def clean_text_for_csv(text: str) -> str:
    """
    å¢å¼ºç‰ˆæ–‡æœ¬æ¸…ç†ã€‚
    ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåˆå¹¶å¤šä½™ç©ºç™½ï¼Œå¹¶è¿›è¡ŒCSVè½¬ä¹‰ï¼Œé™åˆ¶é•¿åº¦é˜²æ­¢Excelå•å…ƒæ ¼æº¢å‡ºã€‚
    """
    if not text or not isinstance(text, str):
        return ""
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦ï¼Œæ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'[\n\r\f\v\x0b\x0c\t]+', ' ', text)
    text = re.sub(r'[\x00-\x08\x0e-\x1f\x7f-\x9f]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # CSVè½¬ä¹‰ï¼šå°†åŒå¼•å·æ›¿æ¢ä¸ºä¸¤ä¸ªåŒå¼•å·
    text = text.replace('"', '""')
    
    # é™åˆ¶é•¿åº¦ (Excelå•å…ƒæ ¼é€šå¸¸é™åˆ¶åœ¨32767å­—ç¬¦)
    if len(text) > 32000:
        text = text[:32000] + "...[æ–‡æœ¬è¢«æˆªæ–­]"
    
    return text

def contains_ai_governance_keywords(text: str) -> str:
    """æ£€æµ‹AIæ²»ç†ç›¸å…³å…³é”®è¯å¹¶è¿”å›åŒ¹é…ä¿¡æ¯"""
    if not text or not isinstance(text, str):
        return "æ— æ³•åˆ†æ"
    
    text_lower = text.lower()
    # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„å…³é”®è¯
    matched_keywords = [k for k in Config.AI_GOVERNANCE_KEYWORDS if k.lower() in text_lower]
    
    if len(matched_keywords) >= 3:
        return f"é«˜åº¦ç›¸å…³ (åŒ¹é…{len(matched_keywords)}ä¸ªå…³é”®è¯: {', '.join(matched_keywords[:3])}...)"
    elif len(matched_keywords) >= 2:
        return f"ç›¸å…³ (åŒ¹é…{len(matched_keywords)}ä¸ªå…³é”®è¯: {', '.join(matched_keywords)})"
    elif len(matched_keywords) == 1:
        return f"å¯èƒ½ç›¸å…³ (åŒ¹é…å…³é”®è¯: {matched_keywords[0]})"
    else:
        return "ä¸ç›¸å…³"

# --- PDFå’Œæ–‡æ¡£å¤„ç†å¢å¼º (Document Processing) ---
def is_valid_pdf_url(url: str) -> bool:
    """åˆ¤æ–­URLæ˜¯å¦å¯èƒ½æ˜¯PDFæ–‡æ¡£"""
    pdf_indicators = [
        '.pdf', 'filetype=pdf', 'content-type=pdf', '/pdf/', 'document', 'download'
    ]
    url_lower = url.lower()
    return any(indicator in url_lower for indicator in pdf_indicators)

def get_file_info_from_response(response: requests.Response) -> Dict:
    """ä»HTTPå“åº”ä¸­æå–æ–‡ä»¶ä¿¡æ¯"""
    content_type = response.headers.get('content-type', '').lower()
    content_disposition = response.headers.get('content-disposition', '')
    content_length = response.headers.get('content-length', '0')
    
    # æå–æ–‡ä»¶å
    filename = None
    if 'filename=' in content_disposition:
        # ä½¿ç”¨æ›´ç¨³å¥çš„æ­£åˆ™åŒ¹é…æ–‡ä»¶å
        filename_match = re.search(r'filename\*?=["\']?([^"\';\n]+)', content_disposition)
        if filename_match:
            # unquote å¤„ç†URLç¼–ç çš„æ–‡ä»¶å
            filename = unquote(filename_match.group(1).encode('latin-1').decode('utf-8', 'ignore'))
    
    return {
        'content_type': content_type,
        'filename': filename,
        'size_bytes': int(content_length) if content_length.isdigit() else 0,
        # æ£€æŸ¥ Content-Type æˆ–å†…å®¹å‰4ä¸ªå­—èŠ‚æ˜¯å¦ä¸º %PDF
        'is_pdf': 'pdf' in content_type or response.content[:4] == b'%PDF'
    }

@retry(stop=stop_after_attempt(Config.MAX_RETRIES), wait=wait_exponential(multiplier=1, min=2, max=10), 
       retry_error_callback=lambda retry_state: (None, f"æ–‡æ¡£ä¸‹è½½æœ€ç»ˆå¤±è´¥: {retry_state.outcome.exception()}", {}))
def download_document_smart(url: str, session: requests.Session, output_dir: Path, 
                          url_index: Any, page_info: Dict = None) -> Tuple[Optional[Path], Optional[str], Dict]:
    """
    æ™ºèƒ½æ–‡æ¡£ä¸‹è½½ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œå¸¦é‡è¯•æœºåˆ¶ã€‚
    url_index å¯ä»¥æ˜¯æ•°å­—æˆ–å­—ç¬¦ä¸²ï¼Œç”¨äºæ–‡ä»¶åç”Ÿæˆã€‚
    """
    if page_info is None:
        page_info = {}
        
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # ä½¿ç”¨æµå¼GETè¯·æ±‚ï¼Œé¿å…å¤§æ–‡ä»¶ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
        response = session.get(url, headers=headers, timeout=Config.PDF_DOWNLOAD_TIMEOUT, 
                             stream=True, allow_redirects=True)
        
        if response.status_code != 200:
            # å°è¯•å¤„ç†é‡å®šå‘åçš„URL
            final_url = response.url
            if final_url != url:
                 response = session.get(final_url, headers=headers, timeout=Config.PDF_DOWNLOAD_TIMEOUT, 
                                     stream=True, allow_redirects=True)
                 if response.status_code != 200:
                    return None, f"HTTPçŠ¶æ€ç : {response.status_code} (æœ€ç»ˆURL)", {}
            else:
                return None, f"HTTPçŠ¶æ€ç : {response.status_code}", {}

        file_info = get_file_info_from_response(response)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if file_info['size_bytes'] > Config.MAX_PDF_SIZE_MB * 1024 * 1024:
            response.close()
            return None, f"æ–‡ä»¶è¿‡å¤§: {file_info['size_bytes']/1024/1024:.1f}MB", file_info
        
        # ç”Ÿæˆæ–‡ä»¶å
        # ä¼˜å…ˆçº§ï¼šæ”¿ç­–æ ‡é¢˜ > Content-Dispositionæ–‡ä»¶å > é»˜è®¤åç§°
        policy_title = page_info.get('policy_title', '')
        if policy_title:
            base_name = generate_safe_filename(policy_title)[:50]
        elif file_info.get('filename'):
            # ç§»é™¤æ–‡ä»¶æ‰©å±•åï¼Œä½¿ç”¨ generate_safe_filename
            name_part = Path(file_info['filename']).stem
            base_name = generate_safe_filename(name_part)[:50]
        else:
            base_name = f"document_{url_index}"
        
        # ç¡®å®šæ–‡ä»¶æ‰©å±•å
        if file_info['is_pdf']:
            extension = '.pdf'
        elif 'html' in file_info['content_type']:
            extension = '.html'
        elif 'xml' in file_info['content_type']:
            extension = '.xml'
        else:
            # å°è¯•ä»åŸå§‹æ–‡ä»¶åæ¨æ–­æ‰©å±•å
            ext_from_url = Path(urlparse(url).path).suffix.lower()
            extension = ext_from_url if ext_from_url in ['.pdf', '.doc', '.docx', '.txt', '.rtf'] else '.pdf'
            
        # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€æ€§
        filename = f"{url_index}_{base_name}{extension}"
        file_path = output_dir / filename

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
        if file_path.exists() and file_path.stat().st_size > 1024:
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
            # æ­¤æ—¶éœ€è¦é‡æ–°æ„å»ºfile_infoï¼Œå› ä¸ºæ˜¯ä»ç£ç›˜è¯»å–
            file_info['size_bytes'] = file_path.stat().st_size
            return file_path, None, file_info

        # ä¸‹è½½æ–‡ä»¶
        total_size = 0
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    total_size += len(chunk)

        response.close() # å¿…é¡»å…³é—­è¿æ¥
        
        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶å¤§å°
        if total_size == 0:
            file_path.unlink(missing_ok=True)
            return None, "ä¸‹è½½æ–‡ä»¶å†…å®¹ä¸ºç©º", file_info

        # PDFé­”æœ¯å­—èŠ‚éªŒè¯
        if extension == '.pdf' and not file_path.read_bytes()[:4] == b'%PDF':
            file_path.unlink(missing_ok=True) # åˆ é™¤æ— æ•ˆæ–‡ä»¶
            return None, "ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„PDF (é­”æœ¯å­—èŠ‚æ£€æŸ¥å¤±è´¥)", file_info

        # ä¿å­˜å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        metadata = {
            'url': url,
            'filename': filename,
            'download_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'file_info': file_info,
            'page_info': page_info or {},
            'actual_size': file_path.stat().st_size
        }
        metadata_path = file_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"æˆåŠŸä¸‹è½½æ–‡æ¡£: {filename} ({file_info['size_bytes']/1024:.1f}KB)")
        return file_path, None, file_info

    except requests.exceptions.Timeout:
        raise
    except requests.exceptions.RequestException as e:
        raise
    except Exception as e:
        logger.error(f"ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {str(e)}")
        return None, f"ä¸‹è½½å¤±è´¥: {str(e)}", {}

def extract_text_from_document(file_path: Path) -> str:
    """ä»å¤šç§æ–‡æ¡£æ ¼å¼ä¸­æå–æ–‡æœ¬"""
    try:
        if file_path.suffix.lower() == '.pdf':
            return extract_pdf_text_robust(file_path)
        elif file_path.suffix.lower() in ['.html', '.htm']:
            return extract_html_text(file_path)
        elif file_path.suffix.lower() == '.xml':
            return extract_xml_text(file_path)
        else:
            # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å– (å¤„ç†.doc/.docxéœ€è¦é¢å¤–åº“ï¼Œæ­¤å¤„ä»…åšç®€å•æ–‡æœ¬è¯»å–)
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
    except Exception as e:
        return f"[ERROR] æ–‡æœ¬æå–å¤±è´¥: {str(e)}"

def extract_pdf_text_robust(pdf_path: Path) -> str:
    """å¢å¼ºç‰ˆPDFæ–‡æœ¬æå–ï¼šå°è¯•å¤šç§ç­–ç•¥"""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            texts = []
            
            for page_num, page in enumerate(pdf.pages):
                page_text = ""
                
                # ç­–ç•¥ 1: æ ‡å‡†æ–‡æœ¬æå– (æœ€å¸¸ç”¨)
                try:
                    page_text = page.extract_text()
                except Exception:
                    pass
                
                if not page_text or len(page_text.strip()) < 10:
                    # ç­–ç•¥ 2: å¸ƒå±€ä¿æŒæå– (ä¿ç•™æ›´ç²¾ç¡®çš„å¸ƒå±€)
                    try:
                        page_text = page.extract_text(layout=True, x_tolerance=2, y_tolerance=2)
                    except Exception:
                        pass
                
                if not page_text or len(page_text.strip()) < 10:
                    # ç­–ç•¥ 3: è¡¨æ ¼æå– (è¡¥å……è¡¨æ ¼å†…å®¹)
                    try:
                        tables = page.extract_tables()
                        if tables:
                            table_texts = []
                            for table in tables:
                                table_text = "\n".join([
                                    " | ".join([str(cell) if cell else "" for cell in row])
                                    for row in table
                                ])
                                table_texts.append(table_text)
                            # å°†è¡¨æ ¼å†…å®¹è¿½åŠ åˆ°å½“å‰æ–‡æœ¬
                            if texts:
                                page_text = texts.pop().split('\n\n')[0] + "\n\n" + "\n\n".join(table_texts) # å°è¯•ä¸ä¸Šä¸€é¡µåˆå¹¶
                            else:
                                page_text = "\n\n".join(table_texts)
                    except Exception:
                        pass

                if page_text and len(page_text.strip()) > 5:
                    texts.append(f"[é¡µé¢ {page_num + 1}]\n{page_text}")
                
            full_text = "\n\n".join(texts)
            
            if not full_text.strip():
                return f"[WARNING] PDFè§£ææˆåŠŸä½†æœªèƒ½æå–æœ‰æ•ˆæ–‡æœ¬ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–å›¾åƒPDF"
            
            return full_text
            
    except Exception as e:
        return f"[ERROR] PDFè§£æå¤±è´¥: {str(e)}"

def extract_html_text(html_path: Path) -> str:
    """ä»HTMLæ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
        
        # ç§»é™¤è„šæœ¬ã€æ ·å¼å’Œå¯¼èˆªç­‰éä¸»è¦å†…å®¹
        for element in soup(["script", "style", "nav", "header", "footer", "aside", "form", "button"]):
            element.decompose()
        
        return soup.get_text(strip=True, separator=' ')
    except Exception as e:
        return f"[ERROR] HTMLæ–‡æœ¬æå–å¤±è´¥: {str(e)}"

def extract_xml_text(xml_path: Path) -> str:
    """ä»XMLæ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        with open(xml_path, 'r', encoding='utf-8', errors='ignore') as f:
            soup = BeautifulSoup(f.read(), 'xml')
        return soup.get_text(strip=True, separator=' ')
    except Exception as e:
        return f"[ERROR] XMLæ–‡æœ¬æå–å¤±è´¥: {str(e)}"

# --- Seleniumå’Œæµè§ˆå™¨ç®¡ç† (Selenium and Browser Management) ---
def find_chromedriver_path() -> Optional[str]:
    """è‡ªåŠ¨æŸ¥æ‰¾ChromeDriverè·¯å¾„"""
    for path in Config.CHROMEDRIVER_PATHS:
        if Path(path).exists() and os.access(path, os.X_OK):
            logger.info(f"æ‰¾åˆ°ChromeDriver: {path}")
            return path
    
    logger.warning("æœªæ‰¾åˆ°ChromeDriverï¼Œè¯·å®‰è£…æˆ–é…ç½®è·¯å¾„")
    return None

def init_chrome_driver_stealth() -> Optional[webdriver.Chrome]:
    """åˆå§‹åŒ–éšèº«ç‰ˆChrome Driverï¼Œå®Œå…¨æ— å¤´æ¨¡å¼ï¼ŒåŒ…å«åæ£€æµ‹é…ç½®"""
    chromedriver_path = find_chromedriver_path()
    if not chromedriver_path:
        return None
    
    logger.info("ğŸš€ åˆå§‹åŒ–éšèº«Chromeæµè§ˆå™¨...")
    
    options = webdriver.ChromeOptions()
    
    # åŸºç¡€éšèº«é…ç½®
    options.add_argument("--headless=new")  # ä½¿ç”¨æ–°çš„æ— å¤´æ¨¡å¼
    options.add_argument("--no-sandbox") # Linuxç¯å¢ƒå¿…å¤‡
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-plugins")
    # options.add_argument("--disable-images")  # ä¸åŠ è½½å›¾ç‰‡ï¼ˆå¯é€‰ï¼Œå¯ä»¥åŠ é€Ÿä½†å¯èƒ½å½±å“åŠ¨æ€å†…å®¹ï¼‰
    # options.add_argument("--disable-javascript")  # å¯é€‰ï¼šç¦ç”¨JSä¼šå½±å“åŠ¨æ€ç½‘ç«™
    
    # åæ£€æµ‹é…ç½®
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    
    # çª—å£å’Œæ˜¾ç¤ºé…ç½®
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-notifications")
    
    # éšæœºç”¨æˆ·ä»£ç†
    options.add_argument(f"--user-agent={random.choice(USER_AGENTS)}")
    
    # ä¸‹è½½é…ç½®ï¼ˆé˜²æ­¢PDFåœ¨æµè§ˆå™¨å†…æ‰“å¼€ï¼Œå¹¶è®¾ç½®ä¸‹è½½ç›®å½•ï¼‰
    download_prefs = {
        "download.default_directory": str(Config.PDF_SAVE_DIR.absolute()),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "plugins.always_open_pdf_externally": True, # å…³é”®ï¼šè®©PDFç›´æ¥ä¸‹è½½
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_settings.popups": 0
    }
    options.add_experimental_option("prefs", download_prefs)
    
    # æ—¥å¿—é…ç½®
    options.add_argument("--log-level=3")  # åªæ˜¾ç¤ºè‡´å‘½é”™è¯¯
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    
    try:
        service = Service(executable_path=chromedriver_path)
        driver = webdriver.Chrome(service=service, options=options)
        
        # è®¾ç½®è¶…æ—¶
        driver.set_page_load_timeout(Config.PAGE_LOAD_TIMEOUT)
        driver.implicitly_wait(10) # éšå¼ç­‰å¾…
        
        # åæ£€æµ‹è„šæœ¬ï¼šç§»é™¤webdriveræ ‡å¿—
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        logger.info("âœ… éšèº«Chromeæµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver
        
    except Exception as e:
        logger.error(f"âŒ Chromeæµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def handle_comprehensive_popups(driver: webdriver.Chrome) -> bool:
    """å…¨é¢å¤„ç†å„ç§å¼¹çª—ï¼šcookiesã€éšç§ã€è®¢é˜…ã€å¹¿å‘Šç­‰"""
    handled_popup = False
    attempt_count = 0
    
    # å®šä¹‰å„ç§å¼¹çª—å¤„ç†è§„åˆ™ï¼ˆä½¿ç”¨XPATHå’ŒCSS Selectorï¼‰
    popup_handlers = [
        # Cookie åŒæ„/æ¥å— æŒ‰é’®
        {
            'name': 'CookieåŒæ„',
            'selectors': [
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'accept')]",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'agree')]",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'allow')]",
                ".cookie-accept", "#cookie-accept", ".accept-cookies", "#accept-cookies", 
                ".cookie-banner button", ".cookie-consent button", ".gdpr-accept", ".consent-accept"
            ]
        },
        # å…³é—­å¼¹çª— (X æŒ‰é’®) æˆ– â€œç¨åâ€/â€œä¸ï¼Œè°¢è°¢â€
        {
            'name': 'å…³é—­/è·³è¿‡',
            'selectors': [
                "//button[contains(@class, 'close')]", "//span[contains(@class, 'close')]",
                "//button[@aria-label='Close']", ".modal-close", ".popup-close", ".dialog-close",
                "[aria-label='Close']", "[data-dismiss='modal']",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'no thanks')]",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'skip')]",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'later')]",
                "//button[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'not now')]",
                ".newsletter-dismiss", ".subscription-close", ".newsletter-close"
            ]
        },
    ]
    
    while attempt_count < Config.MAX_POPUP_ATTEMPTS and not handled_popup:
        attempt_count += 1
        
        for handler in popup_handlers:
            for selector in handler['selectors']:
                try:
                    # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´æ¥å¿«é€Ÿæ£€æµ‹å¼¹çª—
                    if selector.startswith("//"):
                        # XPATH æŸ¥æ‰¾
                        elements = WebDriverWait(driver, Config.POPUP_DETECTION_TIMEOUT).until(
                            EC.presence_of_all_elements_located((By.XPATH, selector))
                        )
                    else:
                        # CSS Selector æŸ¥æ‰¾
                        elements = WebDriverWait(driver, Config.POPUP_DETECTION_TIMEOUT).until(
                            EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))
                        )
                    
                    # å°è¯•ç‚¹å‡»æ‰€æœ‰åŒ¹é…çš„å…ƒç´ 
                    for element in elements:
                        if element.is_displayed() and element.is_enabled():
                            try:
                                # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
                                driver.execute_script("arguments[0].scrollIntoView(true);", element)
                                time.sleep(0.5)
                                
                                # å°è¯•æ™®é€šç‚¹å‡»
                                element.click()
                                logger.info(f"âœ… æˆåŠŸå¤„ç†{handler['name']}å¼¹çª— (æ™®é€šç‚¹å‡»)")
                                handled_popup = True
                                time.sleep(1)
                                break
                                
                            except ElementClickInterceptedException:
                                # å¦‚æœç‚¹å‡»è¢«æ‹¦æˆªï¼Œå°è¯•JSç‚¹å‡»
                                try:
                                    driver.execute_script("arguments[0].click();", element)
                                    logger.info(f"âœ… æˆåŠŸå¤„ç†{handler['name']}å¼¹çª— (JSç‚¹å‡»)")
                                    handled_popup = True
                                    time.sleep(1)
                                    break
                                except Exception as js_error:
                                    logger.debug(f"JSç‚¹å‡»ä¹Ÿå¤±è´¥: {js_error}")
                                    continue
                            except Exception as click_error:
                                logger.debug(f"ç‚¹å‡»å¤±è´¥: {click_error}")
                                continue
                        
                        if handled_popup:
                            break
                            
                except TimeoutException:
                    continue
                except Exception as e:
                    logger.debug(f"å¼¹çª—å¤„ç†å¼‚å¸¸: {e}")
                    continue
            
            if handled_popup:
                break
        
        # è¿™ä¸€è½®æ²¡æœ‰å¤„ç†åˆ°å¼¹çª—ï¼Œç¨å¾®ç­‰å¾…ä¸€ä¸‹å†å°è¯•
        if not handled_popup and attempt_count < Config.MAX_POPUP_ATTEMPTS:
            time.sleep(1)
    
    # é¢å¤–å°è¯•ï¼šæŒ‰ESCé”®å…³é—­å¯èƒ½çš„å¼¹çª—
    if not handled_popup and attempt_count == Config.MAX_POPUP_ATTEMPTS:
        try:
            from selenium.webdriver.common.keys import Keys
            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨ESCé”®å…³é—­å¼¹çª—")
            time.sleep(1)
            handled_popup = True # å‡è®¾ESCç”Ÿæ•ˆ
        except Exception:
            pass
    
    return handled_popup

def find_ai_related_links(soup: BeautifulSoup, base_url: str) -> List[Dict]:
    """æ™ºèƒ½å‘ç°AIç›¸å…³çš„å­é¡µé¢é“¾æ¥ï¼Œç”¨äºæ™ºèƒ½å¯¼èˆª"""
    ai_links = []
    
    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        text = link.get_text(strip=True).lower()
        title = link.get('title', '').lower()
        
        if not href or href.startswith('#'):
            continue
            
        full_url = urljoin(base_url, href)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„HTTPé“¾æ¥ä¸”ä¸åœ¨åŒä¸€é¡µé¢
        if not full_url.startswith(('http://', 'https://')):
            continue
        
        # ç¡®ä¿æ˜¯åŒä¸€ä¸ªåŸŸåä¸‹çš„é“¾æ¥ï¼ˆé˜²æ­¢è·³å‡ºç½‘ç«™ï¼‰
        if urlparse(full_url).netloc != urlparse(base_url).netloc:
            continue
            
        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ã€æ ‡é¢˜æˆ–URLæ˜¯å¦åŒ…å«AIå…³é”®è¯
        relevance_score = 0
        matched_keywords = []
        
        # ä½¿ç”¨Configä¸­çš„AIå…³é”®è¯
        for keyword in Config.AI_GOVERNANCE_KEYWORDS:
            if keyword in text or keyword in title or keyword in href.lower():
                relevance_score += 1
                matched_keywords.append(keyword)
        
        # åªé€‰æ‹©ç›¸å…³æ€§è¾ƒé«˜çš„é“¾æ¥
        if relevance_score > 0:
            # æ’é™¤å›¾ç‰‡ã€é‚®ä»¶ã€ç”µè¯ç­‰éé¡µé¢é“¾æ¥
            if any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.mailto', '.tel']):
                 continue
                 
            ai_links.append({
                'url': full_url,
                'text': link.get_text(strip=True)[:100],
                'title': link.get('title', '')[:100],
                'relevance_score': relevance_score,
                'matched_keywords': matched_keywords
            })
    
    # æŒ‰ç›¸å…³æ€§æ’åºå¹¶å»é‡
    ai_links = sorted(ai_links, key=lambda x: x['relevance_score'], reverse=True)
    seen_urls = set()
    unique_links = []
    
    for link in ai_links:
        # ç§»é™¤URLæœ«å°¾çš„'/'å·®å¼‚
        normalized_url = link['url'].rstrip('/')
        if normalized_url not in seen_urls:
            seen_urls.add(normalized_url)
            unique_links.append(link)
    
    # é™åˆ¶è¿”å›æ•°é‡
    return unique_links[:Config.MAX_AI_LINKS_PER_PAGE]

def smart_navigate_and_extract(driver: webdriver.Chrome, url: str, max_depth: int) -> Tuple[List[str], List[Dict], List[str]]:
    """æ™ºèƒ½å¯¼èˆªå’Œæå–ï¼šè‡ªåŠ¨è·³è½¬AIç›¸å…³å­é¡µé¢"""
    extracted_texts = []
    visited_urls = set()
    documents_info = []
    navigation_log = []
    
    def log_and_append(message):
        logger.info(message)
        navigation_log.append(message)

    def extract_from_page(current_url: str, depth: int = 0) -> None:
        """é€’å½’æå–å­å‡½æ•°"""
        # é˜»æ­¢æ¡ä»¶ï¼šè¾¾åˆ°æœ€å¤§æ·±åº¦ã€å·²è®¿é—®è¿‡
        if depth > max_depth or current_url.rstrip('/') in visited_urls:
            return
            
        visited_urls.add(current_url.rstrip('/'))
        log_and_append(f"ğŸ” æ­£åœ¨åˆ†æé¡µé¢ (æ·±åº¦ {depth}): {current_url}")
        
        try:
            # å¯¼èˆªåˆ°é¡µé¢
            driver.get(current_url)
            time.sleep(random.uniform(2, 3)) # å¯¼èˆªåç­‰å¾…
            
            # å¤„ç†å¼¹çª—å’ŒåŠ¨æ€åŠ è½½
            handle_page_interactions(driver, current_url)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            WebDriverWait(driver, 10).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # 1. æŸ¥æ‰¾æ–‡æ¡£é“¾æ¥
            doc_links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                link_text = a_tag.get_text(strip=True).lower()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æ¡£é“¾æ¥
                if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.txt', '.rtf']) or \
                   any(keyword in href.lower() for keyword in ['download', 'document', 'file', 'attachment']) or \
                   any(keyword in link_text for keyword in ['download', 'pdf', 'document', 'read more']):
                    
                    full_doc_url = urljoin(current_url, href)
                    doc_links.append({
                        'url': full_doc_url,
                        'text': a_tag.get_text(strip=True),
                        'type': 'document'
                    })
            
            documents_info.extend(doc_links)
            
            # 2. æå–å½“å‰é¡µé¢çš„æ–‡æœ¬å†…å®¹
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(["script", "style", "nav", "header", "footer", "aside", 
                               "form", "button", "img", ".navigation", ".menu", ".sidebar"]):
                if element:
                    element.decompose()
            
            # å¯»æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = None
            content_selectors = [
                'article', 'main', '.main-content', '.content', '.policy-content',
                '#main-content', '#content', '.document-content', '.text-content',
                '.post-content', '.entry-content', 'body'
            ]
            
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content and len(main_content.get_text(strip=True)) > Config.MIN_CONTENT_LENGTH:
                    break
            
            page_text = ""
            if main_content:
                page_text = main_content.get_text(strip=True, separator=' ')
            
            # åªä¿å­˜æœ‰è¶³å¤Ÿå†…å®¹çš„é¡µé¢
            if len(page_text) > Config.MIN_CONTENT_LENGTH:
                formatted_text = (
                    f"[é¡µé¢URL]: {current_url}\n"
                    f"[æå–æ·±åº¦]: {depth}\n"
                    f"[æå–æ—¶é—´]: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                    f"{page_text}"
                )
                extracted_texts.append(formatted_text)
                log_and_append(f"âœ… ä»é¡µé¢æå–æ–‡æœ¬: {len(page_text)} å­—ç¬¦")
            
            # 3. é€’å½’å¯¼èˆªåˆ°AIç›¸å…³å­é“¾æ¥
            if depth < max_depth:
                ai_links = find_ai_related_links(soup, current_url)
                log_and_append(f"ğŸ”— å‘ç° {len(ai_links)} ä¸ªAIç›¸å…³å­é“¾æ¥")
                
                # è®¿é—®å‰Nä¸ªæœ€ç›¸å…³çš„å­é“¾æ¥
                for ai_link in ai_links:
                    normalized_sub_url = ai_link['url'].rstrip('/')
                    if normalized_sub_url not in visited_urls:
                        log_and_append(f"ğŸ¯ è·³è½¬åˆ°AIç›¸å…³é¡µé¢ (å¾—åˆ†{ai_link['relevance_score']}): {ai_link['text'][:50]}...")
                        time.sleep(random.uniform(2, 4))  # éšæœºå»¶è¿Ÿ
                        extract_from_page(ai_link['url'], depth + 1)
                        
        except TimeoutException:
            log_and_append(f"âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶: {current_url}")
        except WebDriverException as e:
            log_and_append(f"âš ï¸ æµè§ˆå™¨æ“ä½œå¤±è´¥ {current_url}: {e}")
        except Exception as e:
            log_and_append(f"âš ï¸ é¡µé¢å¤„ç†å¤±è´¥ {current_url}: {e}")
    
    # å¼€å§‹é€’å½’æå–
    extract_from_page(url, 0)
    
    return extracted_texts, documents_info, navigation_log

def handle_page_interactions(driver: webdriver.Chrome, url: str) -> None:
    """å¤„ç†é¡µé¢äº¤äº’ï¼šcookiesã€å¼¹çª—ã€æ»šåŠ¨ç­‰"""
    try:
        # 1. ä½¿ç”¨ç»¼åˆå¼¹çª—å¤„ç†å‡½æ•°
        handle_comprehensive_popups(driver)
        
        # 2. æ»šåŠ¨é¡µé¢ä»¥è§¦å‘åŠ¨æ€åŠ è½½
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/3);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight*2/3);")
        time.sleep(1)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        # 3. å°è¯•ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½ï¼ˆå¦‚JavaScriptæ¸²æŸ“ï¼‰
        WebDriverWait(driver, 5).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        
    except Exception as e:
        logger.debug(f"é¡µé¢äº¤äº’å¤„ç†å¼‚å¸¸: {e}")

# --- æ ¸å¿ƒå¤„ç†å‡½æ•° (Main Processing Logic) ---
def process_url_comprehensive(url: str, url_index: int, row_data: Dict = None) -> Tuple[str, int, Dict]:
    """
    ç»¼åˆURLå¤„ç†å‡½æ•°ã€‚
    1. æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥PDFã€‚
    2. å¯åŠ¨æ™ºèƒ½å¯¼èˆªï¼ˆSeleniumï¼‰é€’å½’æå–é¡µé¢å†…å®¹å’Œæ–‡æ¡£é“¾æ¥ã€‚
    3. ä¸‹è½½å¹¶æå–å‘ç°çš„æ–‡æ¡£æ–‡æœ¬ã€‚
    4. å›é€€åˆ°ä¼ ç»Ÿç½‘é¡µæ–‡æœ¬æå–ï¼ˆå¦‚æœå‰ä¸¤æ­¥å¤±è´¥ï¼‰ã€‚
    """
    logger.info(f"ğŸŒ å¼€å§‹ç»¼åˆå¤„ç†URL: {url}")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'DNT': '1',
        'Connection': 'keep-alive'
    })
    
    extracted_text = ""
    pdf_docs_count = 0
    processing_info = {
        'url': url,
        'method': 'unknown',
        'documents_found': 0,
        'pages_visited': 0,
        'ai_links_found': 0,
        'success': False
    }
    
    # æå–é¡µé¢ä¿¡æ¯ï¼ˆç”¨äºæ–‡ä»¶åå’Œå…ƒæ•°æ®ï¼‰
    page_info = {
        'country': row_data.get('Country', 'unknown') if row_data else 'unknown',
        'policy_title': str(row_data.get('Policy initiative ID', f"policy_{url_index}")) if row_data else f"policy_{url_index}",
        'source_url': url
    }
    
    # å°è¯• 1: æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥PDFé“¾æ¥
    if is_valid_pdf_url(url):
        logger.info("ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„ç›´æ¥PDFé“¾æ¥ï¼Œå°è¯•ç›´æ¥ä¸‹è½½")
        # ç›´æ¥PDFä¸‹è½½ä½¿ç”¨é‡è¯•æœºåˆ¶
        try:
            doc_path, error, file_info = download_document_smart(url, session, Config.PDF_SAVE_DIR, url_index, page_info)
            if doc_path:
                text = extract_text_from_document(doc_path)
                if not text.startswith("[ERROR]") and len(text.strip()) > 100:
                    extracted_text = f"=== æ–‡æ¡£å†…å®¹ 1 ===\n{text}"
                    pdf_docs_count = 1
                    processing_info.update({
                        'method': 'direct_pdf',
                        'documents_found': 1,
                        'success': True,
                        'file_info': file_info
                    })
                    logger.info("âœ… ç›´æ¥PDFä¸‹è½½å’Œæå–æˆåŠŸ")
                    return clean_text_for_csv(extracted_text), pdf_docs_count, processing_info
                else:
                    logger.warning(f"âš ï¸ ç›´æ¥PDFå†…å®¹æå–é—®é¢˜: {error or 'å†…å®¹è¿‡å°‘'}")
            else:
                 logger.warning(f"âŒ ç›´æ¥PDFä¸‹è½½å¤±è´¥: {error}")
        except RetryError as e:
            logger.error(f"âŒ ç›´æ¥PDFä¸‹è½½é‡è¯•å¤±è´¥: {e.last_attempt.exception()}")
            pass # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ–¹æ³•

    # å°è¯• 2: ä½¿ç”¨æ™ºèƒ½å¯¼èˆªå¤„ç†ç½‘é¡µ
    driver = None
    try:
        with driver_lock: # ä½¿ç”¨é”ä¿æŠ¤ï¼Œé˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶åˆå§‹åŒ–æµè§ˆå™¨
            driver = init_chrome_driver_stealth()
        
        if not driver:
            raise Exception("æ— æ³•åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨")
        
        logger.info("ğŸ¤– å¯åŠ¨æ™ºèƒ½å¯¼èˆªæ¨¡å¼")
        
        page_texts = []
        discovered_docs = []
        navigation_log = []
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æ™ºèƒ½å¯¼èˆª
        if Config.ENABLE_SMART_NAVIGATION:
            page_texts, discovered_docs, navigation_log = smart_navigate_and_extract(
                driver, url, max_depth=Config.MAX_NAVIGATION_DEPTH
            )
        else:
            # ä¼ ç»Ÿå•é¡µå¤„ç†
            driver.get(url)
            handle_page_interactions(driver, url)
            
            # å³ä½¿ç¦ç”¨æ™ºèƒ½å¯¼èˆªï¼Œä¹Ÿå°è¯•æå–å½“å‰é¡µé¢çš„æ–‡æ¡£é“¾æ¥
            soup = BeautifulSoup(driver.page_source, "html.parser")
            doc_links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.txt', '.rtf']):
                    doc_links.append({'url': urljoin(url, href), 'text': a_tag.get_text(strip=True), 'type': 'document'})
            discovered_docs.extend(doc_links)
        
        # ä¿å­˜cookiesåˆ°sessionï¼Œä¾›requestsä¸‹è½½æ–‡æ¡£ä½¿ç”¨
        cookies = driver.get_cookies()
        for cookie in cookies:
            try:
                session.cookies.set(cookie['name'], cookie['value'], domain=cookie.get('domain'))
            except Exception as e:
                logger.debug(f"è®¾ç½®Cookieå¤±è´¥: {e}")
                
        
        processing_info.update({
            'pages_visited': len(page_texts),
            'ai_links_found': len([d for d in discovered_docs if 'ai' in d.get('text', '').lower()]),
            # ä¿®æ­£ï¼šåªè®¡ç®—å”¯ä¸€çš„æ–‡æ¡£URL
            'documents_found': len(set([d['url'] for d in discovered_docs]))
        })
        
        logger.info(f"ğŸ“Š æ™ºèƒ½å¯¼èˆªç»“æœ: è®¿é—®äº†{len(page_texts)}ä¸ªé¡µé¢, å‘ç°{len(discovered_docs)}ä¸ªæ–‡æ¡£")
        
        # ä¸‹è½½å‘ç°çš„æ–‡æ¡£
        successful_texts = []
        if discovered_docs:
            # å»é‡å’Œè¿‡æ»¤æ— æ•ˆé“¾æ¥
            unique_docs = {d['url']:d for d in discovered_docs}.values()
            
            logger.info(f"ğŸ“„ å¼€å§‹ä¸‹è½½ {len(unique_docs)} ä¸ªå‘ç°çš„æ–‡æ¡£...")
            
            # æ’åºï¼šPDFä¼˜å…ˆï¼ŒAIå…³é”®è¯å¤šçš„ä¼˜å…ˆ
            sorted_docs = sorted(unique_docs, key=lambda x: (
                'pdf' in x.get('url', '').lower(),
                len([kw for kw in Config.AI_GOVERNANCE_KEYWORDS if kw.lower() in x.get('text', '').lower()])
            ), reverse=True)
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
            with ThreadPoolExecutor(max_workers=Config.MAX_THREADS) as executor:
                futures = [
                    executor.submit(download_document_smart, doc['url'], session, Config.PDF_SAVE_DIR, 
                                  f"{url_index}_{i}", page_info)
                    for i, doc in enumerate(sorted_docs[:Config.PDF_DOWNLOAD_LIMIT])
                ]
                
                for i, future in enumerate(as_completed(futures)):
                    try:
                        doc_path, error, file_info = future.result()
                        if doc_path:
                            text = extract_text_from_document(doc_path)
                            if not text.startswith("[ERROR]") and len(text.strip()) > 100:
                                successful_texts.append(text)
                                pdf_docs_count += 1
                                logger.info(f"âœ… æ–‡æ¡£ä¸‹è½½å¹¶æå–æˆåŠŸ: {doc_path.name}")
                            else:
                                logger.warning(f"âš ï¸ æ–‡æ¡£å†…å®¹æå–é—®é¢˜: {error or 'å†…å®¹è¿‡å°‘'}")
                        else:
                            logger.warning(f"âŒ æ–‡æ¡£ä¸‹è½½å¤±è´¥: {error}")
                    except Exception as e:
                         logger.error(f"âŒ æ–‡æ¡£ä¸‹è½½å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}")
        
        # ç»„åˆæ‰€æœ‰æå–çš„å†…å®¹
        all_texts = []
        
        # æ·»åŠ æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if successful_texts:
            all_texts.extend([f"=== æ–‡æ¡£å†…å®¹ {i+1} ===\n{text}" for i, text in enumerate(successful_texts)])
            processing_info['method'] = 'smart_navigation_with_docs'
        
        # æ·»åŠ é¡µé¢å†…å®¹
        if page_texts:
            all_texts.extend([f"=== é¡µé¢å†…å®¹ {i+1} ===\n{text}" for i, text in enumerate(page_texts)])
            if not successful_texts:  # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œåˆ™æ ‡è®°ä¸ºé¡µé¢å†…å®¹
                processing_info['method'] = 'smart_navigation_pages'
        
        if all_texts:
            extracted_text = "\n\n--- å†…å®¹åˆ†éš”ç¬¦ ---\n\n".join(all_texts)
            processing_info['success'] = True
            logger.info(f"âœ… æ™ºèƒ½å¯¼èˆªæˆåŠŸ: æå–äº†{len(all_texts)}ä¸ªå†…å®¹å—")
        
        # å°è¯• 3: å¦‚æœæ™ºèƒ½å¯¼èˆªæ²¡æœ‰ç»“æœï¼Œå›é€€åˆ°ä¼ ç»Ÿç½‘é¡µæ–‡æœ¬æå– (ä»…é’ˆå¯¹é¦–é¡µ)
        if not extracted_text and not Config.ENABLE_SMART_NAVIGATION:
            logger.info("ğŸ“ å›é€€åˆ°ä¼ ç»Ÿç½‘é¡µæ–‡æœ¬æå–...")
            
            # å¦‚æœä¹‹å‰æ²¡æœ‰è®¿é—®è¿‡é¦–é¡µï¼Œç°åœ¨è®¿é—®
            if not page_texts:
                driver.get(url)
                handle_comprehensive_popups(driver)
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(["script", "style", "nav", "header", "footer", "aside", 
                               "form", "button", "img", ".navigation", ".menu", ".sidebar"]):
                if element:
                    element.decompose()
            
            # å¯»æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = None
            content_selectors = [
                'article', 'main', '.main-content', '.content', '.policy-content',
                '#main-content', '#content', '.document-content', '.text-content', 'body'
            ]
            
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            webpage_text = ""
            if main_content:
                webpage_text = main_content.get_text(strip=True, separator=' ')
            
            # æ¸…ç†å’Œæ ¼å¼åŒ–ç½‘é¡µæ–‡æœ¬
            if len(webpage_text.strip()) > 200: # åªæœ‰å†…å®¹è¶³å¤Ÿå¤šæ‰ä½¿ç”¨
                extracted_text = (
                    f"[æ¥æºURL]: {url}\n"
                    f"[å›½å®¶]: {page_info['country']}\n"
                    f"[æ”¿ç­–æ ‡é¢˜]: {page_info['policy_title']}\n"
                    f"[æå–æ—¶é—´]: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                    f"{webpage_text}"
                )
                processing_info.update({
                    'method': 'fallback_webpage_text',
                    'success': True
                })
                logger.info("âœ… å›é€€ç½‘é¡µæ–‡æœ¬æå–æˆåŠŸ")
            
    except Exception as e:
        logger.error(f"âŒ URLå¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
        extracted_text = f"[ERROR] URLå¤„ç†å¤±è´¥: {str(e)}"
        processing_info['error'] = str(e)
        
    finally:
        if driver:
            try:
                driver.quit() # ç¡®ä¿å…³é—­æµè§ˆå™¨å®ä¾‹
            except Exception as e:
                logger.warning(f"å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")
    
    # æœ€ç»ˆæ£€æŸ¥å’Œå¤„ç†
    if not extracted_text or extracted_text.startswith("[ERROR]"):
        if not extracted_text:
            extracted_text = f"[ERROR] æ— æ³•ä»URLæå–ä»»ä½•æœ‰æ•ˆå†…å®¹: {url}"
        processing_info['success'] = False
        processing_info['method'] = 'failed'
    elif not extracted_text.startswith("[ERROR]"):
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if len(extracted_text.strip()) < 100:
            extracted_text = f"[WARNING] æå–å†…å®¹è¿‡å°‘ ({len(extracted_text)} å­—ç¬¦): {extracted_text}"
            processing_info['success'] = False
            processing_info['method'] = 'low_content'
        else:
            processing_info['success'] = True
    
    # æ¸…ç†æ–‡æœ¬ï¼Œé¿å…CSVé—®é¢˜
    final_cleaned_text = clean_text_for_csv(extracted_text)
    
    return final_cleaned_text, pdf_docs_count, processing_info

def generate_safe_filename(text: str, max_length: int = 50) -> str:
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼Œç”¨äºæ–‡æœ¬å’ŒPDFæ–‡ä»¶"""
    if not text:
        return "unknown"
    
    # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
    safe_text = re.sub(r'[\\/:*?"<>|.,;=+[\]\n\r\t]', '_', str(text))
    safe_text = re.sub(r'_+', '_', safe_text)  # å¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶ä¸ºä¸€ä¸ª
    safe_text = safe_text.strip('_')
    
    # é™åˆ¶é•¿åº¦
    if len(safe_text) > max_length:
        safe_text = safe_text[:max_length].rstrip('_')
    
    return safe_text if safe_text else "unknown"

def save_processing_results(results: List[Dict], output_path: Path) -> None:
    """ä¿å­˜å¤„ç†ç»“æœåˆ°CSVæ–‡ä»¶"""
    try:
        df = pd.DataFrame(results)
        
        # å®šä¹‰æ–°å¢å’Œé‡è¦åˆ—çš„é¡ºåº
        new_columns = [
            "æå–æ–‡æœ¬", "AIæ²»ç†ç›¸å…³æ€§", "æ–‡ä»¶å", "å¤„ç†çŠ¶æ€", 
            "PDFæ–‡æ¡£æ•°", "å¤„ç†æ—¶é—´(ç§’)", "æ–‡æœ¬é•¿åº¦", "å¤„ç†æ–¹æ³•",
            "è®¿é—®é¡µé¢æ•°", "å‘ç°æ–‡æ¡£æ•°", "AIé“¾æ¥æ•°"
        ]
        
        # ç¡®ä¿æ‰€æœ‰æ–°å¢åˆ—éƒ½å­˜åœ¨
        for col in new_columns:
            if col not in df.columns:
                df[col] = ''
        
        # å°†åŸå§‹åˆ—å’Œæ–°å¢åˆ—åˆå¹¶ï¼Œå¹¶ä¿æŒé¡ºåº
        original_columns = [col for col in df.columns if col not in new_columns]
        # ç§»é™¤é‡å¤åˆ—å¹¶æ’åº
        final_columns = []
        for col in original_columns + new_columns:
            if col not in final_columns:
                final_columns.append(col)
                
        df = df[final_columns]
        
        # ä¿å­˜åˆ°CSV (ä½¿ç”¨ utf-8-sig ç¼–ç ä»¥é¿å…Excelæ‰“å¼€ä¹±ç )
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}", exc_info=True)
        raise

def print_summary_statistics(results: List[Dict], total_time: float) -> None:
    """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_count = len(results)
    # æˆåŠŸå¤„ç†çš„å®šä¹‰ï¼šæå–æ–‡æœ¬ä¸ä»¥ [ERROR] å¼€å¤´
    success_count = sum(1 for r in results if not r.get('æå–æ–‡æœ¬', '').startswith('[ERROR]'))
    pdf_count = sum(r.get('PDFæ–‡æ¡£æ•°', 0) for r in results)
    ai_relevant_count = sum(1 for r in results if 'ç›¸å…³' in r.get('AIæ²»ç†ç›¸å…³æ€§', ''))
    
    # è®¡ç®—å¹³å‡æ–‡æœ¬é•¿åº¦ï¼ˆåªè®¡ç®—æˆåŠŸæå–çš„æ–‡æœ¬ï¼‰
    successful_results = [r for r in results if r.get('æ–‡æœ¬é•¿åº¦', 0) > 0]
    avg_text_length = sum(r.get('æ–‡æœ¬é•¿åº¦', 0) for r in successful_results) / len(successful_results) if successful_results else 0
    
    print("\n" + "=" * 80)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 80)
    print(f"ğŸ“‹ æ€»å¤„ç†æ•°é‡: {total_count}")
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count} ({success_count/total_count*100:.1f}%)")
    print(f"âŒ å¤„ç†å¤±è´¥: {total_count - success_count}")
    print(f"ğŸ“„ ä¸‹è½½PDFæ•°: {pdf_count}")
    print(f"ğŸ¤– AIæ²»ç†ç›¸å…³: {ai_relevant_count} ({ai_relevant_count/total_count*100:.1f}%)")
    print(f"ğŸ“ å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_text_length:.0f} å­—ç¬¦")
    print(f"â±ï¸ æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"âš¡ å¹³å‡å¤„ç†é€Ÿåº¦: {total_time/total_count:.1f} ç§’/ä¸ª")
    print("=" * 80)

# --- ä¸»æ‰§è¡Œå‡½æ•° (Main Execution) ---
def main_worker(df: pd.DataFrame, url_column: str, total_urls: int, start_time: float) -> List[Dict]:
    """ä¸»é€»è¾‘çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºçº¿ç¨‹æ± """
    
    # å°†å¤„ç†é€»è¾‘å°è£…åˆ°ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿äºåœ¨å•çº¿ç¨‹ï¼ˆè°ƒè¯•ï¼‰æˆ–å¤šçº¿ç¨‹ï¼ˆç”Ÿäº§ï¼‰ä¸­ä½¿ç”¨
    def process_single_url(idx_original: int, row: pd.Series) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªURLçš„å°è£…å‡½æ•°"""
        url = row[url_column]
        row_dict = row.to_dict()
        
        # ä½¿ç”¨'ç¼–å·'ä½œä¸ºä¸»è¦ç´¢å¼•ï¼Œæˆ–ä½¿ç”¨DataFrameçš„index
        idx = row_dict.get('ç¼–å·', idx_original)
        
        # ç”Ÿæˆæ–‡ä»¶ååŸºç¡€
        if 'Country' in row_dict and 'Policy initiative ID' in row_dict:
            country = generate_safe_filename(str(row_dict.get('Country', 'unknown')))
            policy_id = generate_safe_filename(str(row_dict.get('Policy initiative ID', 'unknown')))
            filename_base = f"{country}-{policy_id}"
        else:
            filename_base = f"{idx:04d}"
        
        filename_txt = f"{filename_base}.txt"
        
        logger.info(f"\n--- [{idx_original + 1}/{total_urls}] å¤„ç†: {filename_base} ---")
        logger.info(f"ğŸ”— URL: {url}")
        
        processing_start = time.time()
        
        # æ ¸å¿ƒå¤„ç†
        extracted_text, pdf_docs_count, processing_info = process_url_comprehensive(
            url, idx, row_dict
        )
        
        processing_time = time.time() - processing_start
        
        # åˆ†æç»“æœ
        if extracted_text.startswith("[ERROR]"):
            status = "å¤±è´¥"
            ai_relevance = "å¤„ç†å¤±è´¥"
            display_text = extracted_text
            text_length = 0
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {extracted_text}")
            
        elif extracted_text.startswith("[WARNING]"):
            status = "è­¦å‘Š"
            ai_relevance = "å†…å®¹è¿‡å°‘"
            display_text = extracted_text
            text_length = len(extracted_text)
            logger.warning(f"âš ï¸ å¤„ç†è­¦å‘Š: {extracted_text}")

        else:
            status = f"æˆåŠŸ-{processing_info.get('method', 'unknown')}"
            if pdf_docs_count > 0:
                status += f"-{pdf_docs_count}æ–‡æ¡£"
            
            # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
            try:
                text_file_path = Config.SAVE_DIR / filename_txt
                with open(text_file_path, "w", encoding="utf-8") as f:
                    f.write(extracted_text)
                logger.info(f"ğŸ’¾ æ–‡æœ¬å·²ä¿å­˜: {filename_txt}")
                
            except Exception as e:
                logger.error(f"âŒ æ–‡æœ¬ä¿å­˜å¤±è´¥: {e}")
                extracted_text = f"[ERROR] æ–‡æœ¬ä¿å­˜å¤±è´¥: {e}"
                status = "å¤±è´¥-ä¿å­˜å¼‚å¸¸"
            
            # åˆ†æAIç›¸å…³æ€§
            ai_relevance = contains_ai_governance_keywords(extracted_text)
            text_length = len(extracted_text)
            
            # å†³å®šæ˜¾ç¤ºå†…å®¹
            if text_length < 1000:  # çŸ­æ–‡æœ¬ç›´æ¥æ˜¾ç¤º
                display_text = extracted_text
            else:  # é•¿æ–‡æœ¬åªæ˜¾ç¤ºæ–‡ä»¶å¼•ç”¨
                display_text = f"æ–‡æœ¬å†…å®¹å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename_txt} (é•¿åº¦: {text_length} å­—ç¬¦)"
            
            logger.info(f"âœ… å¤„ç†æˆåŠŸ (æ–¹æ³•: {processing_info.get('method', 'unknown')})")
            logger.info(f"ğŸ“„ æ–‡æ¡£æ•°: {pdf_docs_count}, ğŸ“ é•¿åº¦: {text_length}, ğŸ¤– ç›¸å…³æ€§: {ai_relevance}")

        # æ”¶é›†ç»“æœ
        result_record = {
            **row_dict,
            "æå–æ–‡æœ¬": display_text,
            "AIæ²»ç†ç›¸å…³æ€§": ai_relevance,
            "æ–‡ä»¶å": filename_txt,
            "å¤„ç†çŠ¶æ€": status,
            "PDFæ–‡æ¡£æ•°": pdf_docs_count,
            "å¤„ç†æ—¶é—´(ç§’)": round(processing_time, 1),
            "æ–‡æœ¬é•¿åº¦": text_length,
            "å¤„ç†æ–¹æ³•": processing_info.get('method', 'unknown'),
            "è®¿é—®é¡µé¢æ•°": processing_info.get('pages_visited', 0),
            "å‘ç°æ–‡æ¡£æ•°": processing_info.get('documents_found', 0),
            "AIé“¾æ¥æ•°": processing_info.get('ai_links_found', 0)
        }
        
        # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬
        delay = random.uniform(Config.RANDOM_DELAY_MIN, Config.RANDOM_DELAY_MAX)
        logger.info(f"ğŸ˜´ ä¼‘æ¯ {delay:.1f} ç§’...")
        time.sleep(delay)
        
        return result_record

    all_results: List[Dict] = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†URL
    with ThreadPoolExecutor(max_workers=Config.MAX_THREADS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_url = {
            executor.submit(process_single_url, idx, row): (idx, row[url_column]) 
            for idx, row in df.iterrows()
        }
        
        success_count = 0
        total_pdf_count = 0
        
        # æ”¶é›†ç»“æœå¹¶æŠ¥å‘Šè¿›åº¦
        for i, future in enumerate(as_completed(future_to_url)):
            idx, url = future_to_url[future]
            
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    
                    # æ›´æ–°è¿›åº¦ä¿¡æ¯
                    if not result.get('æå–æ–‡æœ¬', '').startswith('[ERROR]'):
                        success_count += 1
                    total_pdf_count += result.get('PDFæ–‡æ¡£æ•°', 0)
                    
                    # æ‰“å°è¿›åº¦æŠ¥å‘Š
                    progress = (i + 1) / total_urls * 100
                    elapsed_time = time.time() - start_time
                    remaining_time = (elapsed_time / (i + 1)) * (total_urls - i - 1) / 60
                    
                    print(f"\n--- ğŸ“Š è¿›åº¦æŠ¥å‘Š ---")
                    print(f"ğŸ“Š æ€»ä½“è¿›åº¦: {progress:.1f}% ({i+1}/{total_urls}) | æˆåŠŸ: {success_count} | PDFæ€»æ•°: {total_pdf_count}")
                    if remaining_time > 0:
                        print(f"â±ï¸  é¢„è®¡å‰©ä½™æ—¶é—´: {remaining_time:.1f} åˆ†é’Ÿ")
                    logger.info(f"ğŸ“Š æ€»ä½“è¿›åº¦: {progress:.1f}% | æˆåŠŸ: {success_count} | PDFæ€»æ•°: {total_pdf_count}")
                    
            except Exception as e:
                logger.error(f"âŒ URL {url} çš„å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
                # æ·»åŠ ä¸€ä¸ªå¤±è´¥è®°å½•åˆ°ç»“æœåˆ—è¡¨
                all_results.append({
                    **df.iloc[idx].to_dict(), 
                    "æå–æ–‡æœ¬": f"[ERROR] å¹¶å‘ä»»åŠ¡å¼‚å¸¸: {e}",
                    "AIæ²»ç†ç›¸å…³æ€§": "å¤„ç†å¤±è´¥",
                    "æ–‡ä»¶å": f"{df.iloc[idx].get('ç¼–å·', idx):04d}.txt",
                    "å¤„ç†çŠ¶æ€": "å¤±è´¥-ä»»åŠ¡å¼‚å¸¸",
                    "PDFæ–‡æ¡£æ•°": 0,
                    "å¤„ç†æ—¶é—´(ç§’)": round(time.time() - start_time, 1),
                    "æ–‡æœ¬é•¿åº¦": 0
                })
                
    return all_results

# ... (ä¿æŒæ‰€æœ‰åŸå§‹å¯¼å…¥å’Œé…ç½®ä¸å˜) ...

def main():
    """ä¸»ç¨‹åºå…¥å£ (å·²å¢å¼ºæ–­ç‚¹ç»­çˆ¬åŠŸèƒ½)"""
    start_time = time.time()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("ğŸš€ å¢å¼ºç‰ˆOECD.aiæ–‡æ¡£æŠ“å–å·¥å…· (æ”¯æŒæ–­ç‚¹ç»­çˆ¬)")
    print("=" * 80)
    print("ğŸ”§ å½“å‰é…ç½®:")
    print(f"   ğŸ“ è¾“å…¥æ–‡ä»¶: {Config.EXCEL_PATH}")
    print(f"   ğŸ¤– æ™ºèƒ½å¯¼èˆª: {'å¯ç”¨' if Config.ENABLE_SMART_NAVIGATION else 'ç¦ç”¨'}")
    print(f"   ğŸ” å¯¼èˆªæ·±åº¦: {Config.MAX_NAVIGATION_DEPTH}")
    print(f"   ğŸ”— æ¯é¡µAIé“¾æ¥æ•°: {Config.MAX_AI_LINKS_PER_PAGE}")
    print(f"   ğŸ“„ æœ€å¤§PDFä¸‹è½½: {Config.PDF_DOWNLOAD_LIMIT}")
    print(f"   âš¡ æœ€å¤§çº¿ç¨‹æ•°: {Config.MAX_THREADS}")
    print("=" * 80)
    
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆOECD.aiæ–‡æ¡£æŠ“å–å·¥å…·")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    for path in [Config.SAVE_DIR, Config.PDF_SAVE_DIR, Config.TEMP_DIR]:
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ç¡®ä¿ç›®å½•å­˜åœ¨: {path}")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
            print(f"âŒ é”™è¯¯: åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
            return
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    try:
        df = detect_and_read_file(Config.EXCEL_PATH)
        logger.info(f"ğŸ“– æˆåŠŸè¯»å–æ–‡ä»¶: {Config.EXCEL_PATH}")
        print(f"ğŸ“Š è¯»å–åˆ° {df.shape[0]} è¡Œæ•°æ®ï¼Œ{df.shape[1]} åˆ—")
        
    except Exception as e:
        logger.error(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
        print(f"âŒ é”™è¯¯: è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾URLåˆ—
    url_column = find_url_column(df)
    if not url_column:
        logger.error("âŒ æœªæ‰¾åˆ°URLåˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        print(f"âŒ æœªæ‰¾åˆ°URLåˆ—ï¼å¯ç”¨åˆ—: {list(df.columns)}")
        return
    
    print(f"ğŸ”— ä½¿ç”¨URLåˆ—: {url_column}")
    
    # --- æ–­ç‚¹ç»­çˆ¬æ ¸å¿ƒé€»è¾‘ ---
    processed_urls = set()
    old_results: List[Dict] = []
    df_processed = pd.DataFrame()
    
    if Config.CSV_OUTPUT.exists():
        try:
            # 1. å°è¯•è¯»å–ä¸Šæ¬¡çš„è¾“å‡ºç»“æœ
            df_processed = pd.read_csv(Config.CSV_OUTPUT, encoding="utf-8-sig")
            
            # 2. ç­›é€‰å·²å¤„ç†æˆåŠŸçš„ URL
            processed_url_col = find_url_column(df_processed) 
            
            if processed_url_col and 'å¤„ç†çŠ¶æ€' in df_processed.columns:
                # è®¤ä¸ºåŒ…å« 'æˆåŠŸ' æˆ– 'è­¦å‘Š' çš„çŠ¶æ€ä¸ºå·²å¤„ç†
                success_statuses = ['æˆåŠŸ', 'è­¦å‘Š']
                processed_mask = df_processed['å¤„ç†çŠ¶æ€'].astype(str).str.contains('|'.join(success_statuses), na=False)
                processed_urls = set(df_processed[processed_mask][processed_url_col].astype(str).str.strip().tolist())
                
                # æå–å·²æˆåŠŸå¤„ç†çš„ç»“æœä½œä¸ºæ—§ç»“æœ
                old_results = df_processed[processed_mask].to_dict('records')
                
                logger.info(f"ğŸ’¾ æ£€æµ‹åˆ°ä¸Šæ¬¡è¿è¡Œç»“æœï¼Œå‘ç° {len(processed_urls)} ä¸ªå·²å¤„ç†æˆåŠŸçš„URLã€‚")
                print(f"ğŸ’¾ æ£€æµ‹åˆ°ä¸Šæ¬¡è¿è¡Œç»“æœï¼Œå‘ç° {len(processed_urls)} ä¸ªå·²å¤„ç†æˆåŠŸçš„URLã€‚")
            
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ä¸Šæ¬¡è¾“å‡ºç»“æœå¤±è´¥ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰URL: {e}")
            old_results = []
            processed_urls = set()

    # 3. æ•°æ®é¢„å¤„ç†ï¼šä»åŸå§‹æ•°æ®ä¸­è¿‡æ»¤æ‰å·²å¤„ç†çš„ URL
    original_count = len(df)
    df = df.dropna(subset=[url_column])
    df[url_column] = df[url_column].astype(str).str.strip()
    df = df[df[url_column].str.startswith(('http://', 'https://'))]
    
    # è¿‡æ»¤æ‰å·²æˆåŠŸå¤„ç†çš„ URL
    df_to_process = df[~df[url_column].astype(str).str.strip().isin(processed_urls)].copy()
    
    # 4. è¡¥å……å¤„ç†ä¸Šæ¬¡å¤±è´¥çš„ URL
    if not df_processed.empty:
        # æ‰¾å‡ºä¸Šæ¬¡å¤±è´¥çš„è®°å½•
        failed_mask = ~df_processed['å¤„ç†çŠ¶æ€'].astype(str).str.contains('æˆåŠŸ|è­¦å‘Š', na=False)
        df_failed = df_processed[failed_mask]
        
        if not df_failed.empty:
            failed_urls = set(df_failed[url_column].astype(str).str.strip().tolist())
            
            # æ‰¾åˆ°åŸå§‹æ•°æ®ä¸­å¯¹åº”çš„è¡Œ
            df_to_reprocess = df[df[url_column].astype(str).str.strip().isin(failed_urls)]
            
            # å°†æœªå¤„ç†çš„å’Œå¤±è´¥çš„åˆå¹¶ï¼ˆDataFrameçš„concatä¼šè‡ªåŠ¨å¤„ç†ç´¢å¼•ï¼‰
            df_to_process = pd.concat([df_to_process, df_to_reprocess]).drop_duplicates(subset=[url_column], keep='last')
            logger.info(f"ğŸ”„ é‡æ–°åŠ å…¥ {len(failed_urls)} ä¸ªä¸Šæ¬¡å¤„ç†å¤±è´¥çš„URLè¿›è¡Œé‡è¯•ã€‚")
            print(f"ğŸ”„ é‡æ–°åŠ å…¥ {len(failed_urls)} ä¸ªä¸Šæ¬¡å¤„ç†å¤±è´¥çš„URLè¿›è¡Œé‡è¯•ã€‚")
    
    # æ·»åŠ ç¼–å·åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if 'ç¼–å·' not in df_to_process.columns:
        df_to_process['ç¼–å·'] = range(1, len(df_to_process) + 1)
    
    total_urls_to_process = len(df_to_process)
    
    if total_urls_to_process == 0:
        logger.info("ğŸ‰ æ‰€æœ‰ URL ä¼¼ä¹éƒ½å·²æˆåŠŸå¤„ç†ï¼Œæ²¡æœ‰æ–°çš„å¾…å¤„ç†é¡¹ã€‚")
        print("ğŸ‰ æ‰€æœ‰ URL ä¼¼ä¹éƒ½å·²æˆåŠŸå¤„ç†ï¼Œæ²¡æœ‰æ–°çš„å¾…å¤„ç†é¡¹ã€‚")
        return
    
    logger.info(f"ğŸ“ˆ å¼€å§‹å¤„ç† {total_urls_to_process} ä¸ªå‰©ä½™çš„æœ‰æ•ˆURL...")
    print(f"ğŸ“ˆ å¼€å§‹å¤„ç† {total_urls_to_process} ä¸ªå‰©ä½™çš„æœ‰æ•ˆURL...")
    
    # æ ¸å¿ƒå¤„ç†æµç¨‹
    new_results: List[Dict] = []
    try:
        # å°†å¾…å¤„ç†çš„DataFrameä¼ å…¥ main_worker
        new_results = main_worker(df_to_process, url_column, total_urls_to_process, start_time)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œæ­£åœ¨ä¿å­˜å·²å¤„ç†çš„ç»“æœ...")
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
    
    # 5. åˆå¹¶å¹¶ä¿å­˜æœ€ç»ˆç»“æœ
    
    # æœ€ç»ˆç»“æœæ˜¯ï¼šæ—§çš„æˆåŠŸç»“æœ + æ–°å¤„ç†çš„ç»“æœ (åŒ…æ‹¬æ–°çš„æˆåŠŸå’Œå¤±è´¥)
    results = old_results + new_results
    
    if results:
        try:
            # è½¬æ¢ä¸º DataFrame è¿›è¡Œå»é‡ï¼Œç¡®ä¿æœ€æ–°çš„ç»“æœè¦†ç›–æ—§çš„ï¼ˆåŒ…æ‹¬æ—§çš„å¤±è´¥æˆ–æ—§çš„æˆåŠŸï¼‰
            df_final = pd.DataFrame(results)
            # ä»¥ URL ä¸ºä¾æ®è¿›è¡Œå»é‡ï¼Œä¿ç•™æœ€æ–°ï¼ˆåå¤„ç†ï¼‰çš„ç»“æœ
            df_final = df_final.sort_values(by=url_column, key=lambda x: x.astype(str).str.strip()).drop_duplicates(subset=[url_column], keep='last')
            
            # ç”±äº save_processing_results æœŸæœ›ä¸€ä¸ª List[Dict]ï¼Œæˆ‘ä»¬è½¬å›å»
            final_results_list = df_final.to_dict('records') 
            
            save_processing_results(final_results_list, Config.CSV_OUTPUT)
            print(f"ğŸ’¾ æœ€ç»ˆ {len(df_final)} æ¡ç»“æœå·²åˆå¹¶ä¿å­˜åˆ°: {Config.CSV_OUTPUT}")
        except Exception as e:
            logger.error(f"âŒ åˆå¹¶å’Œä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {e}")
            print(f"âŒ åˆå¹¶å’Œä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {e}")
            
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    total_time = time.time() - start_time
    print_summary_statistics(results, total_time)
    
    # è¾“å‡ºè·¯å¾„ä¿¡æ¯
    print(f"\nğŸ“ è¾“å‡ºç›®å½•ä¿¡æ¯:")
    print(f"   ğŸ“ æ–‡æœ¬æ–‡ä»¶: {Config.SAVE_DIR}")
    print(f"   ğŸ“„ PDFæ–‡ä»¶: {Config.PDF_SAVE_DIR}")
    print(f"   ğŸ“Š ç»“æœCSV: {Config.CSV_OUTPUT}")
    print(f"   ğŸ“‹ æ—¥å¿—æ–‡ä»¶: scraper.log")
    
    logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
    """ä¸»ç¨‹åºå…¥å£"""
    start_time = time.time()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("ğŸš€ å¢å¼ºç‰ˆæ–‡æ¡£æŠ“å–å·¥å…·")
    print("=" * 80)
    print("ğŸ”§ å½“å‰é…ç½®:")
    print(f"   ğŸ“ è¾“å…¥æ–‡ä»¶: {Config.EXCEL_PATH}")
    print(f"   ğŸ¤– æ™ºèƒ½å¯¼èˆª: {'å¯ç”¨' if Config.ENABLE_SMART_NAVIGATION else 'ç¦ç”¨'}")
    print(f"   ğŸ” å¯¼èˆªæ·±åº¦: {Config.MAX_NAVIGATION_DEPTH}")
    print(f"   ğŸ”— æ¯é¡µAIé“¾æ¥æ•°: {Config.MAX_AI_LINKS_PER_PAGE}")
    print(f"   ğŸ“„ æœ€å¤§PDFä¸‹è½½: {Config.PDF_DOWNLOAD_LIMIT}")
    print(f"   âš¡ æœ€å¤§çº¿ç¨‹æ•°: {Config.MAX_THREADS}")
    print("=" * 80)
    
    logger.info("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ–‡æ¡£æŠ“å–å·¥å…·")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    for path in [Config.SAVE_DIR, Config.PDF_SAVE_DIR, Config.TEMP_DIR]:
        try:
            path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ç¡®ä¿ç›®å½•å­˜åœ¨: {path}")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
            print(f"âŒ é”™è¯¯: åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
            return
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    try:
        df = detect_and_read_file(Config.EXCEL_PATH)
        logger.info(f"ğŸ“– æˆåŠŸè¯»å–æ–‡ä»¶: {Config.EXCEL_PATH}")
        print(f"ğŸ“Š è¯»å–åˆ° {df.shape[0]} è¡Œæ•°æ®ï¼Œ{df.shape[1]} åˆ—")
        
    except Exception as e:
        logger.error(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
        print(f"âŒ é”™è¯¯: è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾URLåˆ—
    url_column = find_url_column(df)
    if not url_column:
        logger.error("âŒ æœªæ‰¾åˆ°URLåˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        print(f"âŒ æœªæ‰¾åˆ°URLåˆ—ï¼å¯ç”¨åˆ—: {list(df.columns)}")
        return
    
    print(f"ğŸ”— ä½¿ç”¨URLåˆ—: {url_column}")
    
    # æ•°æ®é¢„å¤„ç†
    original_count = len(df)
    df = df.dropna(subset=[url_column])  # ç§»é™¤ç©ºURLè¡Œ
    df[url_column] = df[url_column].astype(str).str.strip()
    df = df[df[url_column].str.startswith(('http://', 'https://'))]  # åªä¿ç•™æœ‰æ•ˆURL
    
    # æ·»åŠ ç¼–å·åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if 'ç¼–å·' not in df.columns:
        df['ç¼–å·'] = range(1, len(df) + 1)
    
    total_urls = len(df)
    filtered_count = original_count - total_urls
    
    if filtered_count > 0:
        print(f"âš ï¸  è¿‡æ»¤æ‰ {filtered_count} è¡Œæ— æ•ˆæ•°æ® (ç©ºURLæˆ–éHTTP/Så¼€å¤´)")
        
    logger.info(f"ğŸ“ˆ å¼€å§‹å¤„ç† {total_urls} ä¸ªæœ‰æ•ˆURL...")
    print(f"ğŸ“ˆ å¼€å§‹å¤„ç† {total_urls} ä¸ªæœ‰æ•ˆURL...")
    
    if total_urls == 0:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URLè¿›è¡Œå¤„ç†")
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URLè¿›è¡Œå¤„ç†")
        return
    
    # æ ¸å¿ƒå¤„ç†æµç¨‹
    results = []
    try:
        results = main_worker(df, url_column, total_urls, start_time)
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œæ­£åœ¨ä¿å­˜å·²å¤„ç†çš„ç»“æœ...")
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    if results:
        try:
            save_processing_results(results, Config.CSV_OUTPUT)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {Config.CSV_OUTPUT}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    total_time = time.time() - start_time
    print_summary_statistics(results, total_time)
    
    # è¾“å‡ºè·¯å¾„ä¿¡æ¯
    print(f"\nğŸ“ è¾“å‡ºç›®å½•ä¿¡æ¯:")
    print(f"   ğŸ“ æ–‡æœ¬æ–‡ä»¶: {Config.SAVE_DIR}")
    print(f"   ğŸ“„ PDFæ–‡ä»¶: {Config.PDF_SAVE_DIR}")
    print(f"   ğŸ“Š ç»“æœCSV: {Config.CSV_OUTPUT}")
    print(f"   ğŸ“‹ æ—¥å¿—æ–‡ä»¶: scraper.log")
    
    logger.info("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    try:
        # ç¡®ä¿ä¸»ç¨‹åºå¼‚å¸¸ä¹Ÿèƒ½è¢«æ•è·å¹¶è®°å½•
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}", exc_info=True)